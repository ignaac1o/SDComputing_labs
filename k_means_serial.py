# -*- coding: utf-8 -*-
"""k-means_def.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MbvEqyQbmmvYybW88SHX-TgYEJgTrY6u
"""

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
import os
import seaborn as sns
import time
import multiprocessing as mp
from scipy.spatial import distance

# Function to standarize dataset
def standarize_data(data_input):
    return data_input.apply(lambda x: (x-x.mean())/ x.std())

# Function to initialize centroids randomly
def initialize_centroids(k, data):
    """
    This function initialize centroids randomly. In particular, this function choose an observation at random 
    and assigns it as a centroid.
    k: number of clusters.
    data: dataset.
    """
    return data.sample(n=k).reset_index(drop=True)

def dist_eucl(a,b):
  return np.sqrt(np.sum((a-b)**2))

def assign_obs(data, centroids):
    """
    It calculates the distance between centroids and each observation of the dataset. Finally, it assigns each
    observation to the nearest cluster. It returns two lists: a list which contains the assignments and other list 
    with the distance associated. 
    data: dataset.
    centroids: the observations which work as centroids.
    """

    n , k = (data.shape[0], centroids.shape[0])
    assignment , distance_associated = ([],[])

    for observation in range(n):
        distances = []
        distances = [dist_eucl(centroids.iloc[centroid,],data.iloc[observation,]) for centroid in range(k)]

        assignment.append(distances.index(min(distances)))
        distance_associated.append(min(distances))

    return (assignment,distance_associated)

def cluster_iterations(dataset, k, n_iterations):
    '''
    It computes several iterations relocating centroids to achieve the final distribution of clusters.
    data: dataset
    k: number of clusters
    '''

    data = dataset.copy()


    # Initialize centroids and error
    centroids = initialize_centroids(k, data)
    error = []
    it = 0

    for i in range(n_iterations):
      data['centroid'], errors_i = assign_obs(data, centroids)
      error.append(sum(errors_i))
      centroids = data.groupby('centroid').agg('mean').reset_index(drop = True)
      it += 1
      if it > 2 and (round(error[i]) == round(error[i-1])):
          break
      else:
          continue

    data['centroid'], errors_i = assign_obs(data,centroids)
    error.append(sum(errors_i))
    data['error'] = error
    return (error[-1])

# Load the dataset
data_read = pd.read_csv("computers.csv").iloc[:,1:7] # We discard the index (first column) and categorical variables

# Standarize data
data_norm = standarize_data(data_read)

#Elbow Graph
start = time.time()
n = 3
n_it = 5
squares = [sum((cluster_iterations(data_norm,i+1, n_it))['errors']) for i in range(n)]
end = time.time()
print("Time elapsed:",end - start,"seconds")
plt.figure(figsize=(10,5))
plt.title("Elbow Graph")
plt.plot(range(1,n+1), squares)
plt.show()

# k=2, optimal clusters number
# Run the algorithm with k=2
# We measure time with time
k_final = 2
n_it = 5
start = time.time()
data_kmeans = cluster_iterations(data_norm, k_final, n_it)
end = time.time()
print("Time elapsed:",end - start,"seconds")

# Plot the first two dimensions of the clusters
sns.scatterplot(x="price", y="speed", hue="centroid", data=data_kmeans)

# We see the cluster with largest average price
mean = []
for i in range(k_final-1):
  mean.append(np.mean(data_norm['price'].loc[(data_norm.centroid == i)]))
  print("The cluster with the largest average price is", mean.index(max(mean)), ",with mean", min(mean))

# Print a heat map
sns.heatmap(data_norm.groupby('centroid').agg('mean').reset_index(drop = True))

